(window.webpackJsonp=window.webpackJsonp||[]).push([[40],{524:function(s,t,a){"use strict";a.r(t);var n=a(46),e=Object(n.a)({},(function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("h1",{attrs:{id:"tensor的基本操作-合并、分割以及统计"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tensor的基本操作-合并、分割以及统计"}},[s._v("#")]),s._v(" Tensor的基本操作：合并、分割以及统计")]),s._v(" "),a("h2",{attrs:{id:"合并与分割-merge-and-split"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#合并与分割-merge-and-split"}},[s._v("#")]),s._v(" 合并与分割(Merge and split)")]),s._v(" "),a("ul",[a("li",[s._v("tf.concat")]),s._v(" "),a("li",[s._v("tf.split")]),s._v(" "),a("li",[s._v("tf.stack")]),s._v(" "),a("li",[s._v("tf.unstack")])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# importing")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" os\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# changing env log level")]),s._v("\nos"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("environ"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'TF_CPP_MIN_LOG_LEVEL'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2'")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" tensorflow "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" tf\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" layers\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" optimizers\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br")])]),a("h3",{attrs:{id:"concat"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#concat"}},[s._v("#")]),s._v(" concat")]),s._v(" "),a("p",[s._v("concat用于张量的拼接操作。例如：一共有六个班级需要统计成绩。其中第一个人统计前四个班级的成绩，另一个人统计后两个班级的成绩。假设每个班有35人，每个人有八门科目的成绩，那么两个人获得的成绩单的shape应该分别是[4,35,8]和[2,35,8]，拼接后的成绩单的shape应该是[6,35,8]。")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("a "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("35")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nb "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("35")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# The operation of concat is seemingly two steps. 1. broadcast. 2. combine ")]),s._v("\nc "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("concat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("(6, 35, 8)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("另外一个类似的场景是两个人统计一个班级的成绩信息，该班级一共有35名学生，第一个人统计前32名学生的成绩，第二个人统计后3名学生的成绩，拼接后得到全班的总成绩单。")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("a "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("32")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nb "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# The operation of concat is seemingly two steps. 1. broadcast. 2. combine ")]),s._v("\nc "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("concat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("(1, 35, 8)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("当然也有这样的场景：现在有四个班级的人考了总共16门考试，其成绩分别记录在了两张表中，每张表记录了8门成绩。现在要将这些成绩放入同一张表：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("a "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("35")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nb "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("35")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nc "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("concat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("(4, 35, 16)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("请注意这两个场景在运算时的区别是基于哪个维度进行拼接。第一种场景下对第0维度进行拼接，第二种场景下对第一维度进行拼接。")]),s._v(" "),a("ul",[a("li",[s._v("concat的使用限制条件为：出了要拼接的维度的大小可以不等之外其它维度需要相等。")])]),s._v(" "),a("h3",{attrs:{id:"stack"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#stack"}},[s._v("#")]),s._v(" stack")]),s._v(" "),a("p",[s._v("stack用于张量的堆叠操作。例如：现在有两个班级的成绩信息，张量结构为[class,student,scoer]。这两个班级分别属于两个学校，现在要将它们放入一张成绩表中，但是要能区分他们的学校。")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("a "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("35")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nb "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("35")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# add a new dim and combine them")]),s._v("\nc "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("stack"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("(2, 4, 35, 8)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("stack可选要扩展维度的位置，例如，我们希望将学校一列放在最后：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("a "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("35")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nb "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("35")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# add a new dim and combine them")]),s._v("\nc "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("stack"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("(4, 35, 8, 2)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("不过一般习惯上把更大的维度（学校）放在前面。")]),s._v(" "),a("ul",[a("li",[s._v("stack的使用限制条件为shape相等。")])]),s._v(" "),a("h3",{attrs:{id:"unstack"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#unstack"}},[s._v("#")]),s._v(" unstack")]),s._v(" "),a("p",[s._v("对应stack，也有unstack。unstack可以在指定的axis上将tensor打散为该axis的size份")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("a "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("35")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nb "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("35")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# add a new dim and combine them")]),s._v("\nc "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("stack"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"shape of the origin : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# unstack")]),s._v("\na_2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("b_2 "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("unstack"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"after unstack : a2:"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("a_2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('",b2:"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("b_2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("shape of the origin :  (2, 4, 35, 8)\nafter unstack : a2: (4, 35, 8) ,b2: (4, 35, 8)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("h3",{attrs:{id:"split"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#split"}},[s._v("#")]),s._v(" split")]),s._v(" "),a("p",[s._v("unstack的使用场景有限。split的功能更加强大。split大体上有两种用法：")]),s._v(" "),a("ul",[a("li",[s._v('第一种：num_or_size_splits是数字，例如"num_or_size_splits=2"的情况，split会将tensor再指定的axis上分成两半。')])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("a "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("35")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nb "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("35")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# add a new dim and combine them")]),s._v("\nc "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("stack"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"shape of the origin : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# split into two part on axis 0")]),s._v("\nres "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("num_or_size_splits"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"after split into two part, len = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("res"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('", shape = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("res"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('" and "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("res"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("shape of the origin :  (2, 4, 35, 8)\nafter split into two part, len =  2 , shape =  (1, 4, 35, 8)  and  (1, 4, 35, 8)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("ul",[a("li",[s._v('第二种：num_or_size_splits是一个list，例如"num_or_size_splits=[1,2,3]"的情况，split会将tensor在指定的axis上分为这个list的size份，在这里是3份，每份的相对大小分别是1、2、3。')])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("a "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("35")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nb "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("35")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# add a new dim and combine them")]),s._v("\nc "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("stack"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"shape of the origin : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# split into three part on axis 3, relative size = 2 ,2 ,4")]),s._v("\nres "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("num_or_size_splits"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("shape of the origin :  (2, 4, 35, 8)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("h2",{attrs:{id:"数据统计"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#数据统计"}},[s._v("#")]),s._v(" 数据统计")]),s._v(" "),a("ul",[a("li",[s._v("tf.norm：张量范数（一范数、二范数、...、无穷范数）")]),s._v(" "),a("li",[s._v("tf.reduce_min：最小值")]),s._v(" "),a("li",[s._v("tf,reduce_max：最大值")]),s._v(" "),a("li",[s._v("tf.argmin：最小值位置")]),s._v(" "),a("li",[s._v("tf.argmax：最大值位置")]),s._v(" "),a("li",[s._v("tf.equal：张量比较")]),s._v(" "),a("li",[s._v("tf.unique：独特值")])]),s._v(" "),a("h3",{attrs:{id:"tf-norm"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tf-norm"}},[s._v("#")]),s._v(" tf.norm")]),s._v(" "),a("p",[s._v("为了好理解，暂时只讨论向量的范数。向量的二范数的公式为：")]),s._v(" "),a("p",{staticClass:"katex-block"},[a("span",{staticClass:"katex-display"},[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"}},[a("semantics",[a("mrow",[a("msup",[a("mrow"),a("mn",[s._v("2")])],1),a("msqrt",[a("mrow",[a("mi",[s._v("s")]),a("mi",[s._v("u")]),a("msubsup",[a("mi",[s._v("m")]),a("mrow",[a("mi",[s._v("i")]),a("mo",[s._v("=")]),a("mn",[s._v("1")])],1),a("mrow",[a("mi",[s._v("s")]),a("mi",[s._v("i")]),a("mi",[s._v("z")]),a("mi",[s._v("e")])],1)],1),a("msubsup",[a("mi",[s._v("x")]),a("mi",[s._v("i")]),a("mn",[s._v("2")])],1)],1)],1)],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[s._v("^2\\sqrt{sum_{i=1}^{size}x_i^2}\n")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1.84em","vertical-align":"-0.5413250000000001em"}}),a("span",{staticClass:"mord"},[a("span"),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.8641079999999999em"}},[a("span",{staticStyle:{top:"-3.113em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[s._v("2")])])])])])])])]),a("span",{staticClass:"mord sqrt"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"1.298675em"}},[a("span",{staticClass:"svg-align",staticStyle:{top:"-3.8em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3.8em"}}),a("span",{staticClass:"mord",staticStyle:{"padding-left":"1em"}},[a("span",{staticClass:"mord mathnormal"},[s._v("s")]),a("span",{staticClass:"mord mathnormal"},[s._v("u")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal"},[s._v("m")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.8064640000000001em"}},[a("span",{staticStyle:{top:"-2.4231360000000004em","margin-left":"0em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[s._v("i")]),a("span",{staticClass:"mrel mtight"},[s._v("=")]),a("span",{staticClass:"mord mtight"},[s._v("1")])])])]),a("span",{staticStyle:{top:"-3.0448000000000004em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[s._v("s")]),a("span",{staticClass:"mord mathnormal mtight"},[s._v("i")]),a("span",{staticClass:"mord mathnormal mtight"},[s._v("ze")])])])])]),a("span",{staticClass:"vlist-s"},[s._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.27686399999999994em"}},[a("span")])])])])]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal"},[s._v("x")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.7959080000000001em"}},[a("span",{staticStyle:{top:"-2.4231360000000004em","margin-left":"0em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[s._v("i")])])]),a("span",{staticStyle:{top:"-3.0448000000000004em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[s._v("2")])])])]),a("span",{staticClass:"vlist-s"},[s._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.27686399999999994em"}},[a("span")])])])])])])]),a("span",{staticStyle:{top:"-3.2586749999999998em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3.8em"}}),a("span",{staticClass:"hide-tail",staticStyle:{"min-width":"1.02em",height:"1.8800000000000001em"}},[a("svg",{attrs:{width:"400em",height:"1.8800000000000001em",viewBox:"0 0 400000 1944",preserveAspectRatio:"xMinYMin slice"}},[a("path",{attrs:{d:"M983 90\nl0 -0\nc4,-6.7,10,-10,18,-10 H400000v40\nH1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7\ns-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744\nc-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30\nc26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722\nc56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5\nc53.7,-170.3,84.5,-266.8,92.5,-289.5z\nM1001 80h400000v40h-400000z"}})])])])]),a("span",{staticClass:"vlist-s"},[s._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.5413250000000001em"}},[a("span")])])])])])])])])]),s._v(" "),a("p",[s._v("向量的n范数的公式为：")]),s._v(" "),a("p",{staticClass:"katex-block"},[a("span",{staticClass:"katex-display"},[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"}},[a("semantics",[a("mrow",[a("msup",[a("mrow"),a("mi",[s._v("n")])],1),a("msqrt",[a("mrow",[a("mi",[s._v("s")]),a("mi",[s._v("u")]),a("msubsup",[a("mi",[s._v("m")]),a("mrow",[a("mi",[s._v("i")]),a("mo",[s._v("=")]),a("mn",[s._v("1")])],1),a("mrow",[a("mi",[s._v("s")]),a("mi",[s._v("i")]),a("mi",[s._v("z")]),a("mi",[s._v("e")])],1)],1),a("msubsup",[a("mi",[s._v("x")]),a("mi",[s._v("i")]),a("mi",[s._v("n")])],1)],1)],1)],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[s._v("^n\\sqrt{sum_{i=1}^{size}x_i^n}\n")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1.84em","vertical-align":"-0.5413250000000001em"}}),a("span",{staticClass:"mord"},[a("span"),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.7143919999999999em"}},[a("span",{staticStyle:{top:"-3.113em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[s._v("n")])])])])])])])]),a("span",{staticClass:"mord sqrt"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"1.298675em"}},[a("span",{staticClass:"svg-align",staticStyle:{top:"-3.8em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3.8em"}}),a("span",{staticClass:"mord",staticStyle:{"padding-left":"1em"}},[a("span",{staticClass:"mord mathnormal"},[s._v("s")]),a("span",{staticClass:"mord mathnormal"},[s._v("u")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal"},[s._v("m")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.8064640000000001em"}},[a("span",{staticStyle:{top:"-2.4231360000000004em","margin-left":"0em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[s._v("i")]),a("span",{staticClass:"mrel mtight"},[s._v("=")]),a("span",{staticClass:"mord mtight"},[s._v("1")])])])]),a("span",{staticStyle:{top:"-3.0448000000000004em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[s._v("s")]),a("span",{staticClass:"mord mathnormal mtight"},[s._v("i")]),a("span",{staticClass:"mord mathnormal mtight"},[s._v("ze")])])])])]),a("span",{staticClass:"vlist-s"},[s._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.27686399999999994em"}},[a("span")])])])])]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal"},[s._v("x")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.6461920000000001em"}},[a("span",{staticStyle:{top:"-2.4231360000000004em","margin-left":"0em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[s._v("i")])])]),a("span",{staticStyle:{top:"-3.0448000000000004em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[s._v("n")])])])]),a("span",{staticClass:"vlist-s"},[s._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.27686399999999994em"}},[a("span")])])])])])])]),a("span",{staticStyle:{top:"-3.2586749999999998em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3.8em"}}),a("span",{staticClass:"hide-tail",staticStyle:{"min-width":"1.02em",height:"1.8800000000000001em"}},[a("svg",{attrs:{width:"400em",height:"1.8800000000000001em",viewBox:"0 0 400000 1944",preserveAspectRatio:"xMinYMin slice"}},[a("path",{attrs:{d:"M983 90\nl0 -0\nc4,-6.7,10,-10,18,-10 H400000v40\nH1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7\ns-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744\nc-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30\nc26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722\nc56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5\nc53.7,-170.3,84.5,-266.8,92.5,-289.5z\nM1001 80h400000v40h-400000z"}})])])])]),a("span",{staticClass:"vlist-s"},[s._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.5413250000000001em"}},[a("span")])])])])])])])])]),s._v(" "),a("p",[s._v('可以理解为：范数是一个函数，是一个向量到数值的映射。向量之间无法比较大小，进行范数运算之后就能直接比较大小了。再换句话理解，这是一种特殊的"欧氏距离(x)"，可以比较向量到远点的距离(x)（我瞎理解的）。')]),s._v(" "),a("p",[s._v("先来一个小一点的tensor：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("origin "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 二范数")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"origin = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"\\nafter norm: "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("norm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 验证一下二范数的运算方式和我们上面说的是否一致")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"origin = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"\\nafter square-square-aqrt : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("sqrt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("reduce_sum"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("square"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"They are the same"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br")])]),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("origin =  tf.Tensor(\n[[1. 1.]\n [1. 1.]], shape=(2, 2), dtype=float32) \nafter norm:  tf.Tensor(2.0, shape=(), dtype=float32)\norigin =  tf.Tensor(\n[[1. 1.]\n [1. 1.]], shape=(2, 2), dtype=float32) \nafter square-square-aqrt :  tf.Tensor(2.0, shape=(), dtype=float32)\nThey are the same\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br")])]),a("p",[s._v("大一点的tensor：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("origin "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("28")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("28")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# more complex example")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"origin = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"\\nafter norm: "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("norm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 验证一下二范数的运算方式和我们上面说的是否一致")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"origin = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"\\nafter square-square-aqrt : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("sqrt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("reduce_sum"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("square"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"They are the same"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("origin =  (4, 28, 28, 3) \nafter norm:  tf.Tensor(96.99484, shape=(), dtype=float32)\norigin =  (4, 28, 28, 3) \nafter square-square-aqrt :  tf.Tensor(96.99484, shape=(), dtype=float32)\nThey are the same\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("p",[s._v("norm除了可以作用在整个张量上，也可以作用在某一个维度上。大概可以理解为对这个维度进行一次unstack然后再对unstack出来的每一个向量求norm。")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("origin "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("28")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("28")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# norm working on specific axis")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"origin = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"\\nafter norm on axis = 3 : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("norm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("origin =  (4, 28, 28, 3) \nafter norm on axis = 3 :  tf.Tensor(\n[[[1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  ...\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]]\n\n [[1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  ...\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]]\n\n [[1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  ...\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]]\n\n [[1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  ...\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]\n  [1.7320508 1.7320508 1.7320508 ... 1.7320508 1.7320508 1.7320508]]], shape=(4, 28, 28), dtype=float32)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br")])]),a("p",[s._v("除了默认的二范数外，norm也可以求n范数。方法是指定ord参数。例如：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("origin "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ones"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 一范数")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"ord = 1 : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("norm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("ord")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 二范数")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"ord = 2 : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("norm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("ord")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 三范数")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"ord = 3 : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("norm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("ord")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 四范数")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"ord = 4 : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("norm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("ord")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 五范数")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"ord = 5 : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("norm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("ord")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("ord = 1 :  tf.Tensor(4.0, shape=(), dtype=float32)\nord = 2 :  tf.Tensor(2.0, shape=(), dtype=float32)\nord = 3 :  tf.Tensor(1.587401, shape=(), dtype=float32)\nord = 4 :  tf.Tensor(1.4142135, shape=(), dtype=float32)\nord = 5 :  tf.Tensor(1.319508, shape=(), dtype=float32)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("h3",{attrs:{id:"tf-reduce-min-max-mean-sum"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tf-reduce-min-max-mean-sum"}},[s._v("#")]),s._v(" tf.reduce_min / max / mean / sum")]),s._v(" "),a("ul",[a("li",[s._v("tf.reduce_min")]),s._v(" "),a("li",[s._v("tf.reduce_max")]),s._v(" "),a("li",[s._v("tf.reduce_mean")]),s._v(" "),a("li",[s._v("tf.reduce_sum")])]),s._v(" "),a("p",[s._v('其实就是求最小值最大值平均值。名字里带着reduce表明，这个操作会有一个类似"打平"的过程。例如，当不指定axis参数时，一个[10,4]的tensor会被"打平"成一个[40]的"list"并求最大值、最小值....；再如，带有axis=2参数时，一个[10,4,10]的tensor会被"降维"变成一个元素为[10,4]的tensor的list，大小是10，然后对着十个元素进行最大、最小....运算。')]),s._v(" "),a("p",[s._v("在整个tensor上操作：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("origin "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("random"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("normal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"origin = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"\\nreduce_min = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("reduce_min"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"\\nreduce_max = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("reduce_max"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"\\nreduce_mean = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("reduce_mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("origin =  tf.Tensor(\n[[-1.4178391e+00  1.0799263e+00  1.8141992e+00 -2.9427743e-01\n   3.5776252e-01 -6.9446379e-01 -7.1207196e-01  9.6388352e-01\n  -2.1230397e+00  4.8318788e-01]\n [-4.1854006e-01 -2.2664030e-01 -9.8776561e-01  3.3819950e-01\n   2.4363371e-02 -3.2178679e+00 -2.8521428e-01 -5.3039378e-01\n  -1.0285269e+00 -1.2320877e+00]\n [ 6.0093373e-01  1.3320454e-02  9.5860285e-01  1.4495020e+00\n   5.1962131e-01  1.1331964e+00 -1.0149366e+00 -5.1126540e-02\n  -5.0443190e-01  3.9746460e-01]\n [-4.1444901e-01 -1.2171540e+00 -8.4814447e-01  1.4405949e+00\n   7.2787516e-04  1.2379333e+00  1.0925928e+00 -9.9176753e-01\n   3.8999468e-02  1.0164096e+00]], shape=(4, 10), dtype=float32) \nreduce_min =  tf.Tensor(-3.2178679, shape=(), dtype=float32) \nreduce_max =  tf.Tensor(1.8141992, shape=(), dtype=float32) \nreduce_mean =  tf.Tensor(-0.08123293, shape=(), dtype=float32)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br")])]),a("p",[s._v("在某个轴上操作：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("origin "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("random"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("normal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"origin = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"\\nreduce_min on axis 1 = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("reduce_min"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"\\nreduce_max on axis 1 = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("reduce_max"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"\\nreduce_mean on axis 1 = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("reduce_mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("origin =  tf.Tensor(\n[[-3.1204236   0.67563623 -0.9232384   1.1589053   0.8515049  -0.47955766\n  -1.723766    0.12821583 -0.6078169  -0.07115268]\n [-0.03351626  0.5452725   0.4999855  -0.13481826  0.6798329   0.23792107\n  -0.6113948   1.3868407   0.24892737 -0.41333905]\n [-0.9676226  -0.3656622  -0.688232    1.721823    0.6695465  -0.44504106\n   0.90125936  0.5428907   1.4090685  -0.9626962 ]\n [-0.87203074  0.9285623   0.56897074 -1.4624474   1.8943952  -0.5554827\n  -0.8351434  -0.3565093  -1.5708245  -1.1640625 ]], shape=(4, 10), dtype=float32)\n\nreduce_min on axis 1 =  tf.Tensor([-3.1204236 -0.6113948 -0.9676226 -1.5708245], shape=(4,), dtype=float32)\n\nreduce_max on axis 1 =  tf.Tensor([1.1589053 1.3868407 1.721823  1.8943952], shape=(4,), dtype=float32)\n\nreduce_mean on axis 1 =  tf.Tensor([-0.4111693   0.24057117  0.18153341 -0.34245723], shape=(4,), dtype=float32)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br")])]),a("h3",{attrs:{id:"tf-argmax-argmin"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tf-argmax-argmin"}},[s._v("#")]),s._v(" tf.argmax/argmin")]),s._v(" "),a("ul",[a("li",[s._v("tf.argmax")]),s._v(" "),a("li",[s._v("tf.argmin")])]),s._v(" "),a("p",[s._v("用于求最小值和最大值的位置。当不指定axis参数时，默认再维度0上求每个维度下标下的最大、最小值的位置。")]),s._v(" "),a("p",[s._v("argmin")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("origin "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("relu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("random"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("normal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"origin = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 维度0下求最大值位置")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"argmax : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("argmax"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("origin =  tf.Tensor(\n[[ 0.        0.        0.      ]\n [ 0.        0.       54.485176]\n [86.468796 74.000046  0.      ]\n [ 0.       29.033602 69.07481 ]], shape=(4, 3), dtype=float32)\nargmax :  tf.Tensor([2 2 3], shape=(3,), dtype=int64)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br")])]),a("p",[s._v("argmax")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("origin "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("relu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("random"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("normal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"origin = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 维度0下求最大值位置，这里对维度0展开会得到二位的tensor，所以得到的最值得位置也会是二维坐标")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"argmax : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("argmax"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("origin =  tf.Tensor(\n[[[  0.        44.180485]\n  [ 60.554047  54.124874]\n  [ 11.455048  54.916447]]\n\n [[  0.        70.35009 ]\n  [147.33435  110.680046]\n  [  0.        59.37093 ]]\n\n [[  0.        20.160051]\n  [  0.        24.07408 ]\n  [  0.         0.      ]]\n\n [[ 84.53291    0.      ]\n  [131.28426  103.82523 ]\n  [192.91162   31.05115 ]]], shape=(4, 3, 2), dtype=float32)\nargmax :  tf.Tensor(\n[[3 1]\n [1 1]\n [3 1]], shape=(3, 2), dtype=int64)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br")])]),a("h3",{attrs:{id:"tf-equal"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tf-equal"}},[s._v("#")]),s._v(" tf.equal")]),s._v(" "),a("p",[s._v("用于比较")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("a "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("constant"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nb "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("constant"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"a = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('", b = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nresult "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("equal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Equal : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("result"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ncast_to_int "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("reduce_sum"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("cast"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("result"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" dtype"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("int32"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"To int32 : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("cast_to_int"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("a =  tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32) , b =  tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int32)\nEqual :  tf.Tensor([False False False False False], shape=(5,), dtype=bool)\nTo int32 :  tf.Tensor(0, shape=(), dtype=int32)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("tf.equal在精确度计算过上似乎有点用。例如，当有一个测试数据集，你的模型跑出来的预测值和测试数据的y做一次equal，然后cast成一个数字，根据大小可以判断accuracy。（也就是相同的部分是准确预测的）")]),s._v(" "),a("h3",{attrs:{id:"tf-unique"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tf-unique"}},[s._v("#")]),s._v(" tf.unique")]),s._v(" "),a("p",[s._v("tf.unique能得到一个包含tensor中所有元素的“set”，并且得到另一个idx的tensor用户标注每一个元素在得到的set里的下标。例如：")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("origin "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("constant"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nresult"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" idx "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("unique"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"origin = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"\\nunique : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("result"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('", \\nidx : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("idx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("origin =  tf.Tensor([4 2 2 4 3], shape=(5,), dtype=int32) \nunique :  tf.Tensor([4 2 3], shape=(3,), dtype=int32) , \nidx :  tf.Tensor([0 1 1 0 2], shape=(5,), dtype=int32)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("回忆一下tf2基本操作中的gather，我们可以通过得到的结果的得到的idx将它复原")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("origin "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("constant"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nresult"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" idx "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("unique"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"origin = "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("origin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"\\nunique : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("result"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('", \\nidx : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("idx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"using tf.gather : "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("gather"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("result"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("idx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[s._v("输出：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("origin =  tf.Tensor([4 2 2 4 3], shape=(5,), dtype=int32) \nunique :  tf.Tensor([4 2 3], shape=(3,), dtype=int32) , \nidx :  tf.Tensor([0 1 1 0 2], shape=(5,), dtype=int32)\nusing tf.gather :  tf.Tensor([4 2 2 4 3], shape=(5,), dtype=int32)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])])])}),[],!1,null,null,null);t.default=e.exports}}]);