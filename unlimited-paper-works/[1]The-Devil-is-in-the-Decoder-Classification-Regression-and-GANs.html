<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>The Devil is in the Decoder: Classification, Regression and GANs | 工具箱的深度学习记事簿</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/statics/logo.svg">
    <meta name="description" content="这里包含了我从入门到依然在入门的过程中接触到的大部分知识。翻翻目录，也许能找到有用的">
    <meta name="keywords" content="Akasaki,Deep learning,Machine learning,工具箱,工具箱的深度学习记事簿,Akasaki的深度学习记事簿">
    
    <link rel="preload" href="/assets/css/0.styles.90685b8c.css" as="style"><link rel="preload" href="/assets/js/app.a1fbffca.js" as="script"><link rel="preload" href="/assets/js/2.8096074b.js" as="script"><link rel="preload" href="/assets/js/5.48054ee2.js" as="script"><link rel="prefetch" href="/assets/js/10.a686cd20.js"><link rel="prefetch" href="/assets/js/11.19e9baf6.js"><link rel="prefetch" href="/assets/js/12.9212f282.js"><link rel="prefetch" href="/assets/js/13.7d0f7067.js"><link rel="prefetch" href="/assets/js/14.4a36e2d7.js"><link rel="prefetch" href="/assets/js/15.e1769a04.js"><link rel="prefetch" href="/assets/js/16.fd1042ef.js"><link rel="prefetch" href="/assets/js/17.f934604d.js"><link rel="prefetch" href="/assets/js/18.bc63a823.js"><link rel="prefetch" href="/assets/js/19.5adad4c4.js"><link rel="prefetch" href="/assets/js/20.0e6892e6.js"><link rel="prefetch" href="/assets/js/21.884c24bc.js"><link rel="prefetch" href="/assets/js/22.3d7bcd74.js"><link rel="prefetch" href="/assets/js/23.b63f63e0.js"><link rel="prefetch" href="/assets/js/24.aec13a9a.js"><link rel="prefetch" href="/assets/js/25.c5426924.js"><link rel="prefetch" href="/assets/js/26.3eb3ba10.js"><link rel="prefetch" href="/assets/js/27.024a3e04.js"><link rel="prefetch" href="/assets/js/28.2772c9bc.js"><link rel="prefetch" href="/assets/js/29.f8ff87be.js"><link rel="prefetch" href="/assets/js/3.98a4c7dc.js"><link rel="prefetch" href="/assets/js/30.50fb28dc.js"><link rel="prefetch" href="/assets/js/31.e24576ff.js"><link rel="prefetch" href="/assets/js/32.9b004a72.js"><link rel="prefetch" href="/assets/js/33.516e30b3.js"><link rel="prefetch" href="/assets/js/34.fce7f963.js"><link rel="prefetch" href="/assets/js/35.dbf61b38.js"><link rel="prefetch" href="/assets/js/36.f3d6b628.js"><link rel="prefetch" href="/assets/js/37.90bb0caf.js"><link rel="prefetch" href="/assets/js/38.6eac4913.js"><link rel="prefetch" href="/assets/js/39.7f5e2f13.js"><link rel="prefetch" href="/assets/js/4.0620af2a.js"><link rel="prefetch" href="/assets/js/40.716fd348.js"><link rel="prefetch" href="/assets/js/41.bb24042a.js"><link rel="prefetch" href="/assets/js/42.c9635060.js"><link rel="prefetch" href="/assets/js/43.b27f3f72.js"><link rel="prefetch" href="/assets/js/44.621812ce.js"><link rel="prefetch" href="/assets/js/45.17e3feb2.js"><link rel="prefetch" href="/assets/js/46.e72b5883.js"><link rel="prefetch" href="/assets/js/47.92f030b3.js"><link rel="prefetch" href="/assets/js/48.eb61d01c.js"><link rel="prefetch" href="/assets/js/49.40e72ee7.js"><link rel="prefetch" href="/assets/js/50.285dbe55.js"><link rel="prefetch" href="/assets/js/51.bd3fd655.js"><link rel="prefetch" href="/assets/js/52.6e2aafec.js"><link rel="prefetch" href="/assets/js/53.6fdc61bc.js"><link rel="prefetch" href="/assets/js/54.199b7b93.js"><link rel="prefetch" href="/assets/js/55.0eaa332d.js"><link rel="prefetch" href="/assets/js/56.d922b396.js"><link rel="prefetch" href="/assets/js/57.b8b028de.js"><link rel="prefetch" href="/assets/js/6.172b3315.js"><link rel="prefetch" href="/assets/js/7.c978e4d5.js"><link rel="prefetch" href="/assets/js/8.13e9f00e.js"><link rel="prefetch" href="/assets/js/9.b094690e.js">
    <link rel="stylesheet" href="/assets/css/0.styles.90685b8c.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">工具箱的深度学习记事簿</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="https://github.com/VisualDust/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="nav-link external">
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="https://github.com/VisualDust/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="nav-link external">
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第零章：在开始之前</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第一章上：HelloWorld</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第一章下：深度学习基础</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第二章上：卷积神经网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第二章下：经典卷积神经网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第三章上：谈一些计算机视觉方向</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第三章下：一些计算机视觉任务</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录：永远是你的好朋友</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第-1章：TensorFlow编程策略</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第-2章：数字信号处理（DSP）</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>魔法部日志（又名论文阅读日志）</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/unlimited-paper-works/[1]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.html" class="active sidebar-link">The Devil is in the Decoder: Classification, Regression and GANs</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[1]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.html#相关研究-related-works" class="sidebar-link">相关研究（Related works）</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[1]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.html#现存的上采样层设计existing-upsampling-layers" class="sidebar-link">现存的上采样层设计Existing upsampling layers）</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[1]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.html#跨层连接和残差连接方法-skip-connections-and-residual-connections" class="sidebar-link">跨层连接和残差连接方法（Skip connections and residual connections）</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[1]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.html#实验和实验设置-task-and-experimental-setups" class="sidebar-link">实验和实验设置（Task and experimental setups）</a></li></ul></li><li><a href="/unlimited-paper-works/[2]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey.html" class="sidebar-link">Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey</a></li><li><a href="/unlimited-paper-works/[3]Progressive-Semantic-Segmentation.html" class="sidebar-link">Progressive Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[4]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation.html" class="sidebar-link">Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li><a href="/unlimited-paper-works/[5]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection.html" class="sidebar-link">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li><a href="/unlimited-paper-works/[6]DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs.html" class="sidebar-link">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></li><li><a href="/unlimited-paper-works/[7]Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation.html" class="sidebar-link">Rethinking Atrous Convolution for Semantic Image Segmentation</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="the-devil-is-in-the-decoder-classification-regression-and-gans"><a href="#the-devil-is-in-the-decoder-classification-regression-and-gans" class="header-anchor">#</a> The Devil is in the Decoder: Classification, Regression and GANs</h1> <h3 id="这篇笔记的写作者是visualdust。"><a href="#这篇笔记的写作者是visualdust。" class="header-anchor">#</a> 这篇笔记的写作者是<a href="https://github.com/visualDust" target="_blank" rel="noopener noreferrer">VisualDust<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。</h3> <p>这是一篇讲各种各样解码器的论文。<a href="/papers/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.pdf">原论文（The Devil is in the Decoder: Classification, Regression and GANs）</a>。</p> <p>由于“解码器（decoder，有些时候也被称为feature extractor）”的概念与像素级的分类、回归等问题多多少少都有瓜葛。以下是decoder被应用于像素级的任务：</p> <ul><li>分类：语义分割、边缘检测。</li> <li>回归：人体关键点检测、深度预测、着色、超分辨。</li> <li>合成：利用生成对抗网络生成图像等。</li></ul> <p>所以decoder是稠密预测（Dence prediction，像素级别的很多问题都可以叫做稠密的）问题的关键。</p> <p>摘要：</p> <blockquote><p>Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.</p></blockquote> <p>我看了这篇综述受益匪浅，如果有时间的话请阅读<a href="/papers/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.pdf">原作</a>。本文只是对原作阅读的粗浅笔记。</p> <hr> <p>​		语义分割、深度预测等计算机视觉任务往往需要对输入进行逐像素的预测，用于解决此类问题的模块通常由编码器组成。编码器（行为上是下采样的，通常情况下是卷积、池化组成的）在学习高维度特征的同时会降低输入图像的空间分辨率；在这之后是将其恢复原始分辨率的解码器（行为是上采样的，通常情况下是转置卷积等操作组成的）：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>编码器（特征提取器，降低特征图分辨率）---解码器（提高特征图分辨率）
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h2 id="相关研究-related-works"><a href="#相关研究-related-works" class="header-anchor">#</a> 相关研究（Related works）</h2> <p>​		这篇论文主要的内容是针对各种像素级的计算机视觉任务，对各种解码器进行了较为广泛的比较。以下是这篇论文的主要贡献：</p> <ol><li>提出选择不同类型的解码器对效果的影响非常巨大</li> <li>为解码器引入了类似残差（residual connection）的新连接</li> <li>介绍了一种比较新颖的解码器：双线性加和上采样（bilinear additive upsampaling）</li> <li>prediction artifacts（真的没想好怎么翻译）</li></ol> <hr> <p><img src="/assets/img/image-20210502073352860.1e18770a.png" alt="image-20210502073352860"></p> <p>我们将需要逐像素预测的问题成为密集预测（dence prediction）的问题。通常编码器-解码器结构是用于解决这种密集预测问题的：首先，编码器（特征提取器）在增加通道数量的同时降低了图像的空间分辨率（通常为8~32倍）；然后，解码器进行上采样恢复到输入原图大小。从概念上讲，此类解码器可以被视为和编码器相反的操作：一个解码器至少由一个提高空间分辨率的层（通常称为上采样层）以及可能保持空间分辨率的层（例如单位卷积、残差快或是起始块）组成。其中 ，用于保持空间分辨率的层已经有了比较成熟的研究，所以这一篇论文只分析提升空间分辨率的部分。</p> <p>目前在单个计算机视觉领域内使用最多的是转置卷积（transposed convolution），它在分割、深度预测、超分辨重建等任务中都有比较详细的论文进行研究。详见原论文中的相关字段。</p> <p>还有一些为了加快模型速度进行的研究，例如：二维卷积在图像分类和语义分割的背景下被分解成两个一维卷；还有一些比较新颖的堆叠的沙漏结构（似乎也可以叫金字塔结构），它是由堆叠的多个编码器-解码器组成。</p> <h2 id="现存的上采样层设计existing-upsampling-layers"><a href="#现存的上采样层设计existing-upsampling-layers" class="header-anchor">#</a> 现存的上采样层设计Existing upsampling layers）</h2> <h3 id="转置卷积-transposed-convolution"><a href="#转置卷积-transposed-convolution" class="header-anchor">#</a> 转置卷积（Transposed Convolution）</h3> <p>转置卷积是最常用的上采样层，有的时候也被称为“反卷积”或是“上卷积”。在输入和输出的关联关系上，转置卷积可以看作是卷积的一种反向操作，但实际上这并不是严格意义的逆运算，逆运算应该是可以被精确计算的，而转置卷积的计算结果并不是精确结果。转置卷积的一种示意如下图：</p> <p><img src="/assets/img/image-20210502090940399.2b189436.png" alt="image-20210502090940399"></p> <p>如图，常见的转置卷积一般会通过某种方式在输入中填充0，以获得一张更大的特征图，其后使用一个标准的卷积运算获得一个比最初始的输入大一些的特征图作为输出。</p> <h3 id="分解的转置卷积-decomposed-transposed-convolution"><a href="#分解的转置卷积-decomposed-transposed-convolution" class="header-anchor">#</a> 分解的转置卷积（Decomposed transposed convolution）</h3> <p>分解的转置卷积和转置卷积是相似的：</p> <p><img src="/assets/img/image-20210502091343561.54d7eb76.png" alt="image-20210502091343561"></p> <p>只不过分解的转置卷积将主卷积运算分为多个低秩卷积。例如，在图像中，分解的转置卷积通过两个一维的卷积对二维的卷积进行模拟。例如上图中，对于输入，先在行上进行隔行填充，然后使用一维的卷积核进行卷积，再在列上进行隔列填充，再使用一维的卷积核进行卷积。</p> <p>分解的转置卷积严格意义上是转置卷积的子集。</p> <p><img src="/assets/img/image-20210502091954758.8e0cab90.png" alt="image-20210502091954758"></p> <p>如上图，这样做的优势是降低了可训练变量的数量（降低了参数量）。</p> <p>分解的转置卷积已经在inception结构中获得了成功：在ILSVRC2012分类赛中获得了the state of the art的成果。</p> <h3 id="深度到空间的转换-depth-to-space"><a href="#深度到空间的转换-depth-to-space" class="header-anchor">#</a> 深度到空间的转换（Depth-to-space）</h3> <p>这种方法（Depth to space）有时也被称为“subpixel convolution”的基本思路是将特征通道移入空间域：</p> <p><img src="/assets/img/image-20210502092533184.e218ac52.png" alt="image-20210502092533184"></p> <p>如上图，本应堆叠在channel维度的不同特征被融合进一个深度为1的平面特征图。这种方法能够很好地保留空间特征，因为它所做的仅仅是改变它们的位置而不是将它们堆叠进channel，而这种方法的缺点是引入了对齐伪像。为了能够和其他几个上采样方法进行横向对比，这篇论文在进行从深度到空间的转换实验之前的下采样卷积比其他上采样层的输出通道多了四倍。</p> <h3 id="插值法-interpolation"><a href="#插值法-interpolation" class="header-anchor">#</a> 插值法（Interpolation）</h3> <h4 id="最临近插值法-nearest-interpolation"><a href="#最临近插值法-nearest-interpolation" class="header-anchor">#</a> 最临近插值法（Nearest Interpolation）</h4> <p><img src="/assets/img/image-20210502104856704.b341eabd.png" alt="image-20210502104856704"></p> <p>最近邻法不需要计算只需要寻找原图中对应的点，所以最近邻法速度最快，但是会破坏原图像中像素的渐变关系，原图像中的像素点的值是渐变的，但是在新图像中局部破坏了这种渐变关系。</p> <h4 id="线性插值法-linear-interpolation"><a href="#线性插值法-linear-interpolation" class="header-anchor">#</a> 线性插值法（linear interpolation）</h4> <p>线性插值法（单线性插值法）是指使用连接两个已知量的直线来确定在这个两个已知量之间的一个未知量的值的方法。</p> <p><img src="/assets/img/image-20210502110046370.68a50857.png" alt="image-20210502110046370"></p> <p>根据初中的知识，2点求一条直线公式，这是双线性插值所需要的唯一的基础公式。</p> <h4 id="双线性插值-bilinear-interpolation"><a href="#双线性插值-bilinear-interpolation" class="header-anchor">#</a> 双线性插值（Bilinear interpolation）</h4> <p>双线性插值可以理解为进行了两次单线性插值：</p> <p><img src="/assets/img/image-20210502110140524.9e150625.png" alt="image-20210502110140524"></p> <p>先在x方向求2次单线性插值，获得R1(x, y1)、R2(x, y2)两个临时点，再在y方向计算1次单线性插值得出P(x, y)（实际上调换2次轴的方向先y后x也是一样的结果）</p> <h3 id="双线性上采样-卷积-bilinear-upsampling-convolution"><a href="#双线性上采样-卷积-bilinear-upsampling-convolution" class="header-anchor">#</a> 双线性上采样+卷积（Bilinear upsampling + Convolution）</h3> <p>双线性上采样+卷积的意思就是在双线性插值之后进行卷积运算。为了和其他上采样方法比较，这篇论文中假设在上采样之后还要进行额外的卷积运算。这种方法的缺点是占用了大量内存和计算空间：双线性插值会二次增加特征量，但同时保持原来的“信息量”。由于假设了双线性上采样之后接有卷积运算，因此这种方法理论上比转置卷积方法的开销高四倍。</p> <h3 id="双线性上采样-可分离卷积-bilinear-upsampling-separable-convolution"><a href="#双线性上采样-可分离卷积-bilinear-upsampling-separable-convolution" class="header-anchor">#</a> 双线性上采样+可分离卷积（Bilinear upsampling+Separable convolution）</h3> <p>可分离的卷积用于构建简单且同质的网络结构，其结果优于InceptionV3。</p> <p><img src="/assets/img/image-20210502111611626.5d155533.png" alt="image-20210502111611626"></p> <p>如上图：一个可分离的卷积又两个操作组成：一个是对每个通道的卷积，另一个是使用<code>(1x1)</code>卷积核的逐点卷积对通道进行“混合”。</p> <h3 id="双线性加性上采样-bilinear-additive-upsampleing"><a href="#双线性加性上采样-bilinear-additive-upsampleing" class="header-anchor">#</a> 双线性加性上采样（Bilinear additive upsampleing）</h3> <p>这个方法是这篇论文在对上述现存的方法进行了叙述后提出的新方法。</p> <p>该论文建议继续使用双线性上采样，但是该论文还将每N个连续的通道相加，从而将输出降低了N倍：</p> <p><img src="/assets/img/image-20210502112224648.5a166e67.png" alt="image-20210502112224648"></p> <p>如上图，该方法的过程是确定性的，唯一可调的参数是N。虽然这个方法很像是之前说的“深度到空间的转换（Depth-to-space）”，但是这个方法并不会导致空间伪像的出现，也就是不需要考虑对齐操作。</p> <p>在这篇论文的实验中，作者选择参数N的标准是让进行双线性加性上采样后和之前的浮点数相等，这使得这种上采样的性能开销类似于转置卷积。</p> <h2 id="跨层连接和残差连接方法-skip-connections-and-residual-connections"><a href="#跨层连接和残差连接方法-skip-connections-and-residual-connections" class="header-anchor">#</a> 跨层连接和残差连接方法（Skip connections and residual connections）</h2> <h3 id="跨层连接-skip-connections"><a href="#跨层连接-skip-connections" class="header-anchor">#</a> 跨层连接（Skip connections）</h3> <p>跨层连接有时也被叫做跳跃连接。这种方法已经在很多解码器结构中获得成功，并且在很多其他的计算机视觉任务中取得了不错的成绩。</p> <p><img src="/assets/img/image-20210502113600719.14c3a724.png" alt="image-20210502113600719"></p> <p>在这种方法中，解码器的每一层输入有两个来源：第一个是上层解码器得到的输出；第二个是在编码器中输出大小和自身输入大小匹配的一层输出的特征。</p> <h3 id="解码器的残差连接-residual-connections-for-decoders"><a href="#解码器的残差连接-residual-connections-for-decoders" class="header-anchor">#</a> 解码器的残差连接（Residual connections for decoders）</h3> <p>残差连接已经在很多不同的计算机视觉任务中被证明是有效的（来源是ResNet）。但是，残差连接并不能直接应用于解码器：在解码器中，下一层比上一层具有更大的空间分辨率和更少的通道数，这和起初残差被提出时的条件恰好相反。所以该论文提出了一个可以解决这些问题的转换方法：特别是上面提出的双线性加性上采样（Bilinear additive upsampleing）方法将输入转化为所需的空间大小和所需的通道数而无需提供任何额外的参数。其转化的特征包含了原始特征的很多信息。因此，可以使用这种转换方法（不进行额外的卷积）进行转换，然后将转换结果输入到任何上采样层的输出中作为下一个上采样层的输入，从而形成类似残差的连接：</p> <p><img src="/assets/img/image-20210502114838945.9a922a54.png" alt="image-20210502114838945"></p> <p>上图是对这种方法进行的图形化解释。在后面的内容中，这篇论文通过实验证明了这种方法的有效性。</p> <h2 id="实验和实验设置-task-and-experimental-setups"><a href="#实验和实验设置-task-and-experimental-setups" class="header-anchor">#</a> 实验和实验设置（Task and experimental setups）</h2> <p>实验部分请查看<a href="/papers/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.pdf">原论文</a>。</p></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">上次更新时间:</span> <span class="time">5/5/2021, 8:29:12 AM</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/ch-2/[2]periodic-signal.html" class="prev">
        周期信号
      </a></span> <span class="next"><a href="/unlimited-paper-works/[2]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey.html">
        Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.a1fbffca.js" defer></script><script src="/assets/js/2.8096074b.js" defer></script><script src="/assets/js/5.48054ee2.js" defer></script>
  </body>
</html>
