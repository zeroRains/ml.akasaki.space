<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>自动求梯度 | 工具箱的深度学习记事簿</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/statics/logo.svg">
    <meta name="description" content="这里包含了我从入门到依然在入门的过程中接触到的大部分知识。翻翻目录，也许能找到有用的">
    <meta name="keywords" content="Akasaki,Deep learning,Machine learning,工具箱,工具箱的深度学习记事簿,Akasaki的深度学习记事簿">
    
    <link rel="preload" href="/assets/css/0.styles.90685b8c.css" as="style"><link rel="preload" href="/assets/js/app.a1fbffca.js" as="script"><link rel="preload" href="/assets/js/2.8096074b.js" as="script"><link rel="preload" href="/assets/js/46.e72b5883.js" as="script"><link rel="prefetch" href="/assets/js/10.a686cd20.js"><link rel="prefetch" href="/assets/js/11.19e9baf6.js"><link rel="prefetch" href="/assets/js/12.9212f282.js"><link rel="prefetch" href="/assets/js/13.7d0f7067.js"><link rel="prefetch" href="/assets/js/14.4a36e2d7.js"><link rel="prefetch" href="/assets/js/15.e1769a04.js"><link rel="prefetch" href="/assets/js/16.fd1042ef.js"><link rel="prefetch" href="/assets/js/17.f934604d.js"><link rel="prefetch" href="/assets/js/18.bc63a823.js"><link rel="prefetch" href="/assets/js/19.5adad4c4.js"><link rel="prefetch" href="/assets/js/20.0e6892e6.js"><link rel="prefetch" href="/assets/js/21.884c24bc.js"><link rel="prefetch" href="/assets/js/22.3d7bcd74.js"><link rel="prefetch" href="/assets/js/23.b63f63e0.js"><link rel="prefetch" href="/assets/js/24.aec13a9a.js"><link rel="prefetch" href="/assets/js/25.c5426924.js"><link rel="prefetch" href="/assets/js/26.3eb3ba10.js"><link rel="prefetch" href="/assets/js/27.024a3e04.js"><link rel="prefetch" href="/assets/js/28.2772c9bc.js"><link rel="prefetch" href="/assets/js/29.f8ff87be.js"><link rel="prefetch" href="/assets/js/3.98a4c7dc.js"><link rel="prefetch" href="/assets/js/30.50fb28dc.js"><link rel="prefetch" href="/assets/js/31.e24576ff.js"><link rel="prefetch" href="/assets/js/32.9b004a72.js"><link rel="prefetch" href="/assets/js/33.516e30b3.js"><link rel="prefetch" href="/assets/js/34.fce7f963.js"><link rel="prefetch" href="/assets/js/35.dbf61b38.js"><link rel="prefetch" href="/assets/js/36.f3d6b628.js"><link rel="prefetch" href="/assets/js/37.90bb0caf.js"><link rel="prefetch" href="/assets/js/38.6eac4913.js"><link rel="prefetch" href="/assets/js/39.7f5e2f13.js"><link rel="prefetch" href="/assets/js/4.0620af2a.js"><link rel="prefetch" href="/assets/js/40.716fd348.js"><link rel="prefetch" href="/assets/js/41.bb24042a.js"><link rel="prefetch" href="/assets/js/42.c9635060.js"><link rel="prefetch" href="/assets/js/43.b27f3f72.js"><link rel="prefetch" href="/assets/js/44.621812ce.js"><link rel="prefetch" href="/assets/js/45.17e3feb2.js"><link rel="prefetch" href="/assets/js/47.92f030b3.js"><link rel="prefetch" href="/assets/js/48.eb61d01c.js"><link rel="prefetch" href="/assets/js/49.40e72ee7.js"><link rel="prefetch" href="/assets/js/5.48054ee2.js"><link rel="prefetch" href="/assets/js/50.285dbe55.js"><link rel="prefetch" href="/assets/js/51.bd3fd655.js"><link rel="prefetch" href="/assets/js/52.6e2aafec.js"><link rel="prefetch" href="/assets/js/53.6fdc61bc.js"><link rel="prefetch" href="/assets/js/54.199b7b93.js"><link rel="prefetch" href="/assets/js/55.0eaa332d.js"><link rel="prefetch" href="/assets/js/56.d922b396.js"><link rel="prefetch" href="/assets/js/57.b8b028de.js"><link rel="prefetch" href="/assets/js/6.172b3315.js"><link rel="prefetch" href="/assets/js/7.c978e4d5.js"><link rel="prefetch" href="/assets/js/8.13e9f00e.js"><link rel="prefetch" href="/assets/js/9.b094690e.js">
    <link rel="stylesheet" href="/assets/css/0.styles.90685b8c.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">工具箱的深度学习记事簿</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="https://github.com/VisualDust/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="nav-link external">
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="https://github.com/VisualDust/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="nav-link external">
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第零章：在开始之前</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>第一章上：HelloWorld</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/ch1p1/[1]operate-on-data.html" class="sidebar-link">数据操作</a></li><li><a href="/ch1p1/[2]automatic-gradient.html" class="active sidebar-link">自动求梯度</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/ch1p1/[2]automatic-gradient.html#简单示例" class="sidebar-link">简单示例</a></li><li class="sidebar-sub-header"><a href="/ch1p1/[2]automatic-gradient.html#训练模式和预测模式" class="sidebar-link">训练模式和预测模式</a></li><li class="sidebar-sub-header"><a href="/ch1p1/[2]automatic-gradient.html#对python控制流求梯度" class="sidebar-link">对Python控制流求梯度</a></li></ul></li><li><a href="/ch1p1/[3]linear-regression.html" class="sidebar-link">线性回归</a></li><li><a href="/ch1p1/[4]linear-regression-code.html" class="sidebar-link">线性回归代码实现</a></li><li><a href="/ch1p1/[5]softmax-regression.html" class="sidebar-link">softmax回归</a></li><li><a href="/ch1p1/[6]softmax-regression-code.html" class="sidebar-link">softmax回归的代码实现</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第一章下：深度学习基础</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第二章上：卷积神经网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第二章下：经典卷积神经网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第三章上：谈一些计算机视觉方向</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第三章下：一些计算机视觉任务</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录：永远是你的好朋友</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第-1章：TensorFlow编程策略</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第-2章：数字信号处理（DSP）</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>魔法部日志（又名论文阅读日志）</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="自动求梯度"><a href="#自动求梯度" class="header-anchor">#</a> 自动求梯度</h1> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">print</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span>

<span class="token comment"># import os</span>
<span class="token comment"># os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'</span>
<span class="token comment"># tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None )</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><div class="language- extra-class"><pre><code>2.0.0
</code></pre></div><p>在深度学习中，我们经常需要对函数求梯度（gradient）。本节将介绍如何使用tensorflow2.0提供的GradientTape来自动求梯度。</p> <h2 id="简单示例"><a href="#简单示例" class="header-anchor">#</a> 简单示例</h2> <p>我们先看一个简单例子：对函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mn>2</mn><msup><mi mathvariant="bold-italic">x</mi><mi mathvariant="normal">⊤</mi></msup><mi mathvariant="bold-italic">x</mi></mrow><annotation encoding="application/x-tex">y = 2\boldsymbol{x}^{\top}\boldsymbol{x}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span></span></span></span> 求关于列向量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">x</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{x}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.44444em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span></span></span></span> 的梯度。我们先创建变量<code>x</code>，并赋初值。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>x <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
x
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><div class="language- extra-class"><pre><code>&lt;tf.Tensor: id=10, shape=(4, 1), dtype=float32, numpy=
array([[0.],
       [1.],
       [2.],
       [3.]], dtype=float32)&gt;
</code></pre></div><p>函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mn>2</mn><msup><mi mathvariant="bold-italic">x</mi><mi mathvariant="normal">⊤</mi></msup><mi mathvariant="bold-italic">x</mi></mrow><annotation encoding="application/x-tex">y = 2\boldsymbol{x}^{\top}\boldsymbol{x}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span></span></span></span> 关于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">x</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{x}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.44444em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span></span></span></span> 的梯度应为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mi mathvariant="bold-italic">x</mi></mrow><annotation encoding="application/x-tex">4\boldsymbol{x}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">4</span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span></span></span></span>。现在我们来验证一下求出来的梯度是正确的。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> t<span class="token punctuation">:</span>
    t<span class="token punctuation">.</span>watch<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    y <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span>
    
dy_dx <span class="token operator">=</span> t<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>y<span class="token punctuation">,</span> x<span class="token punctuation">)</span>
dy_dx
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><div class="language- extra-class"><pre><code>&lt;tf.Tensor: id=30, shape=(4, 1), dtype=float32, numpy=
array([[ 0.],
       [ 4.],
       [ 8.],
       [12.]], dtype=float32)&gt;
</code></pre></div><h2 id="训练模式和预测模式"><a href="#训练模式和预测模式" class="header-anchor">#</a> 训练模式和预测模式</h2> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span>persistent<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">as</span> g<span class="token punctuation">:</span>
    g<span class="token punctuation">.</span>watch<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    y <span class="token operator">=</span> x <span class="token operator">*</span> x
    z <span class="token operator">=</span> y <span class="token operator">*</span> y
    dz_dx <span class="token operator">=</span> g<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>z<span class="token punctuation">,</span> x<span class="token punctuation">)</span>  <span class="token comment"># 108.0 (4*x^3 at x = 3)</span>
    dy_dx <span class="token operator">=</span> g<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>y<span class="token punctuation">,</span> x<span class="token punctuation">)</span>  <span class="token comment"># 6.0</span>
dz_dx<span class="token punctuation">,</span>dy_dx
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><div class="language- extra-class"><pre><code>WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.
WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.

(&lt;tf.Tensor: id=41, shape=(4, 1), dtype=float32, numpy=
 array([[  0.],
        [  4.],
        [ 32.],
        [108.]], dtype=float32)&gt;,
 &lt;tf.Tensor: id=47, shape=(4, 1), dtype=float32, numpy=
 array([[0.],
        [2.],
        [4.],
        [6.]], dtype=float32)&gt;)
</code></pre></div><h2 id="对python控制流求梯度"><a href="#对python控制流求梯度" class="header-anchor">#</a> 对Python控制流求梯度</h2> <p>即使函数的计算图包含了Python的控制流（如条件和循环控制），我们也有可能对变量求梯度。</p> <p>考虑下面程序，其中包含Python的条件和循环控制。需要强调的是，这里循环（while循环）迭代的次数和条件判断（if语句）的执行都取决于输入a的值。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">:</span>
    b <span class="token operator">=</span> a <span class="token operator">*</span> <span class="token number">2</span>
    <span class="token keyword">while</span> tf<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>b<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">1000</span><span class="token punctuation">:</span>
        b <span class="token operator">=</span> b <span class="token operator">*</span> <span class="token number">2</span>
    <span class="token keyword">if</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>b<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
        c <span class="token operator">=</span> b
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        c <span class="token operator">=</span> <span class="token number">100</span> <span class="token operator">*</span> b
    <span class="token keyword">return</span> c
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>我们来分析一下上面定义的<code>f</code>函数。事实上，给定任意输入<code>a</code>，其输出必然是 <code>f(a) = x * a</code>的形式，其中标量系数<code>x</code>的值取决于输入<code>a</code>。由于<code>c = f(a)</code>有关<code>a</code>的梯度为<code>x</code>，且值为<code>c / a</code>，我们可以像下面这样验证对本例中控制流求梯度的结果的正确性。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>a <span class="token operator">=</span> tf<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
<span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> t<span class="token punctuation">:</span>
    t<span class="token punctuation">.</span>watch<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    c <span class="token operator">=</span> f<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
t<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>c<span class="token punctuation">,</span>a<span class="token punctuation">)</span> <span class="token operator">==</span> c<span class="token operator">/</span>a
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><div class="language- extra-class"><pre><code>&lt;tf.Tensor: id=201, shape=(1, 1), dtype=bool, numpy=array([[ True]])&gt;</code></pre></div></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">上次更新时间:</span> <span class="time">5/5/2021, 8:29:12 AM</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/ch1p1/[1]operate-on-data.html" class="prev">
        数据操作
      </a></span> <span class="next"><a href="/ch1p1/[3]linear-regression.html">
        线性回归
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.a1fbffca.js" defer></script><script src="/assets/js/2.8096074b.js" defer></script><script src="/assets/js/46.e72b5883.js" defer></script>
  </body>
</html>
